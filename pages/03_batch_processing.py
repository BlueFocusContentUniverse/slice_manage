import streamlit as st
import os
import tempfile
import uuid
from datetime import datetime
import time
import pandas as pd
import yaml
import glob
import json  # æ·»åŠ jsonå¯¼å…¥ç”¨äºæ–‡ä»¶æŒä¹…åŒ–
import math
import subprocess
from tasks import process_video_task
from celery_app import app
from celery.result import AsyncResult
import logging

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å®šä¹‰å†å²è®°å½•æ–‡ä»¶è·¯å¾„
HISTORY_FILE = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data', 'batch_history.json')

# å®šä¹‰è§†é¢‘åˆ†ç‰‡å¤§å°ï¼ˆ200MBï¼‰
CHUNK_SIZE_MB = 200
CHUNK_SIZE_BYTES = CHUNK_SIZE_MB * 1024 * 1024

# ç¡®ä¿dataç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ‰¹é‡å¤„ç† - è§†é¢‘è§£æå¤„ç†ç³»ç»Ÿ",
    page_icon="ğŸ“¦",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è°ƒè¯•ï¼šæ‰“å°Broker URL (ç§»åŠ¨åˆ° set_page_config ä¹‹å)
#st.write(f"æ‰¹é‡å¤„ç†é¡µé¢ - Broker URL: {app.conf.broker_url}")

# åŠ è½½é…ç½®
def load_config():
    with open('config.yaml', 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)

# åˆ›å»ºä¸´æ—¶ç›®å½•å‡½æ•°
def ensure_temp_dir():
    temp_dir = os.path.join(tempfile.gettempdir(), "streamlit_batch_uploads")
    os.makedirs(temp_dir, exist_ok=True)
    return temp_dir

# å°†ä¸Šä¼ çš„æ–‡ä»¶ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
def save_uploaded_file(uploaded_file):
    temp_dir = ensure_temp_dir()
    file_path = os.path.join(temp_dir, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

# æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°å¹¶å†³å®šæ˜¯å¦éœ€è¦åˆ†ç‰‡
def check_file_size(file_path):
    file_size = os.path.getsize(file_path)
    return file_size > CHUNK_SIZE_BYTES, file_size

# ä½¿ç”¨FFmpegåˆ†å‰²è§†é¢‘æ–‡ä»¶
def split_video_file(file_path, file_name):
    # åˆ›å»ºå­˜æ”¾åˆ†ç‰‡çš„ç›®å½•
    chunks_dir = os.path.join(ensure_temp_dir(), f"chunks_{uuid.uuid4()}")
    os.makedirs(chunks_dir, exist_ok=True)
    
    # è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
    duration_cmd = f'ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "{file_path}"'
    duration = float(subprocess.check_output(duration_cmd, shell=True).decode('utf-8').strip())
    
    # è®¡ç®—æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    file_size = os.path.getsize(file_path)
    
    # è®¡ç®—æ¯ç§’è§†é¢‘çš„å¹³å‡å¤§å°
    bytes_per_second = file_size / duration
    
    # è®¡ç®—æ¯ä¸ªåˆ†ç‰‡çš„æ—¶é•¿ï¼ˆç§’ï¼‰
    chunk_duration = CHUNK_SIZE_BYTES / bytes_per_second
    
    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªåˆ†ç‰‡
    num_chunks = math.ceil(duration / chunk_duration)
    
    chunk_files = []
    
    # ä½¿ç”¨FFmpegåˆ†å‰²è§†é¢‘
    for i in range(num_chunks):
        start_time = i * chunk_duration
        # æœ€åä¸€ä¸ªåˆ†ç‰‡å¯èƒ½ä¸è¶³chunk_duration
        if i == num_chunks - 1:
            chunk_file = os.path.join(chunks_dir, f"{os.path.splitext(file_name)[0]}_part{i+1}{os.path.splitext(file_name)[1]}")
            cmd = f'ffmpeg -y -i "{file_path}" -ss {start_time} -c copy "{chunk_file}"'
        else:
            chunk_file = os.path.join(chunks_dir, f"{os.path.splitext(file_name)[0]}_part{i+1}{os.path.splitext(file_name)[1]}")
            cmd = f'ffmpeg -y -i "{file_path}" -ss {start_time} -t {chunk_duration} -c copy "{chunk_file}"'
        
        # æ‰§è¡Œåˆ†å‰²å‘½ä»¤
        subprocess.run(cmd, shell=True, check=True)
        chunk_files.append((f"{os.path.splitext(file_name)[0]}_part{i+1}{os.path.splitext(file_name)[1]}", chunk_file))
    
    return chunk_files

# ä»æ–‡ä»¶åŠ è½½å†å²è®°å½•
def load_task_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            st.error(f"åŠ è½½å†å²è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
            return []
    return []

# ä¿å­˜å†å²è®°å½•åˆ°æ–‡ä»¶
def save_task_history_to_file(history):
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ä¿å­˜å†å²è®°å½•æ–‡ä»¶å¤±è´¥: {e}")

# ä¿å­˜ä»»åŠ¡å†å²è®°å½•
def save_task_history(task_info):
    if 'batch_task_history' not in st.session_state:
        st.session_state.batch_task_history = load_task_history()
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
    updated = False
    for i, task in enumerate(st.session_state.batch_task_history):
        if task['task_id'] == task_info['task_id']:
            # æ›´æ–°ç°æœ‰ä»»åŠ¡
            st.session_state.batch_task_history[i] = task_info
            updated = True
            break
    
    # æ·»åŠ æ–°ä»»åŠ¡
    if not updated:
        st.session_state.batch_task_history.append(task_info)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    save_task_history_to_file(st.session_state.batch_task_history)

# è·å–ä»»åŠ¡çŠ¶æ€
def get_task_status(task_id):
    """
    ä»Celeryè·å–ä»»åŠ¡çŠ¶æ€ï¼Œè¿™æ˜¯çŠ¶æ€çš„å”¯ä¸€å¯é æ¥æº
    """
    try:
        result = AsyncResult(task_id)
        # è·å–è¯¦ç»†è¿›åº¦ä¿¡æ¯
        status = result.state
        return status, result.info if status == 'PROGRESS' else None
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return "UNKNOWN", None

# åˆå§‹åŒ–session_state
if 'config' not in st.session_state:
    st.session_state.config = load_config()
if 'batch_task_history' not in st.session_state:
    st.session_state.batch_task_history = load_task_history()
if 'batch_processing' not in st.session_state:
    st.session_state.batch_processing = False
if 'batch_tasks' not in st.session_state:
    st.session_state.batch_tasks = []

# ä¾§è¾¹æ 
st.sidebar.title("è§†é¢‘è§£æå¤„ç†ç³»ç»Ÿ")
st.sidebar.info("æ‰¹é‡å¤„ç†é¡µé¢ï¼Œå¯ä»¥ä¸Šä¼ å¤šä¸ªè§†é¢‘å¹¶æ‰¹é‡å¤„ç†")

# ä¸»æ ‡é¢˜
st.title("æ‰¹é‡å¤„ç†")

# ä¸¤åˆ—å¸ƒå±€
col1, col2 = st.columns([3, 1])

with col1:
    st.subheader("æ‰¹é‡ä¸Šä¼ ")
    
    with st.form("batch_upload_form"):
        # çŸ¥è¯†åº“IDè¾“å…¥
        knowledge_base_id = st.text_input(
            "çŸ¥è¯†åº“ID (å¿…å¡«)", 
            value=st.session_state.config['KnowledgeBase']['datasetId'],
            placeholder="è¾“å…¥çŸ¥è¯†åº“ID", 
            key="batch_kb_id"
        )
        
        # è§†é¢‘ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ è§†é¢‘æ–‡ä»¶ (å¤šé€‰)", 
            type=["mp4", "avi", "mov"], 
            accept_multiple_files=True,
            key="batch_video_upload"
        )
        
        # è§£æç»´åº¦æ¨¡æ¿é€‰æ‹©
        dimension_templates = {
            "æ±½è½¦è§†é¢‘è§£æ": """1. **äººç‰©åŠ¨ä½œ**ï¼šå¼€è½¦ã€ä¸‹è½¦ã€ä¸Šè½¦ã€è¡Œèµ°ã€å¼€è½¦é—¨ã€æ•²è½¦é—¨ã€è¯´è¯ç­‰ç­‰ã€‚
2. **è½¦è¾†å±€éƒ¨æè¿°**ï¼šè½¦å¤´ç‰¹å†™ã€è½¦å°¾ç‰¹å†™ã€ä¾§é¢ç‰¹å†™ã€è½®èƒç‰¹å†™ã€è½¦ç¯ç‰¹å†™ã€å†…é¥°ç‰¹å†™ç­‰ã€‚
3. **è½¦è¾†çŠ¶æ€æè¿°**ï¼šåŠ é€Ÿã€åˆ¹è½¦ã€è½¬å¼¯ã€æ¼‚ç§»ã€è¶Šé‡ã€ç¨³å®šå·¡èˆªã€ç–¾é©°è€Œè¿‡ç­‰ã€‚
4. **ç›¸æœºè§†è§’**ï¼šä¿¯è§†è§†è§’ã€ä»°è§†è§†è§’ã€å¹³è§†è§†è§’ã€é¸Ÿç°è§†è§’ç­‰ã€‚
5. **åœ°ç‚¹**ï¼šåŸå¸‚è¡—é“ã€é«˜é€Ÿå…¬è·¯ã€ä¹¡æ‘é“è·¯ã€æµ·æ»¨å…¬è·¯ç­‰ã€‚
6. **æ—¶é—´**ï¼šæ¸…æ™¨ã€ä¸Šåˆã€ä¸­åˆã€ä¸‹åˆã€é»„æ˜ã€å¤œæ™šã€‚
7. **å“ç‰Œè½¦å‹**ï¼š{è½¦å‹}""",
            
            "æ‰‹æœºè§†é¢‘è§£æ": """1. **äººç‰©åŠ¨ä½œ**ï¼šæ‰“ç”µè¯ã€å‘ä¿¡æ¯ã€æµè§ˆç½‘é¡µã€æ‹ç…§ã€å½•åƒç­‰ã€‚
2. **æ‰‹æœºå±€éƒ¨æè¿°**ï¼šå±å¹•ç‰¹å†™ã€æ‘„åƒå¤´ç‰¹å†™ã€è¾¹æ¡†ç‰¹å†™ã€æŒ‰é”®ç‰¹å†™ç­‰ã€‚
3. **æ‰‹æœºçŠ¶æ€æè¿°**ï¼šå¼€æœºã€å…³æœºã€å……ç”µã€è¿è¡Œåº”ç”¨ã€æ˜¾ç¤ºç•Œé¢ç­‰ã€‚
4. **ç›¸æœºè§†è§’**ï¼šä¿¯è§†è§†è§’ã€ä»°è§†è§†è§’ã€å¹³è§†è§†è§’ç­‰ã€‚
5. **åœ°ç‚¹**ï¼šå®¤å†…ã€åŠå…¬å®¤ã€æˆ·å¤–ã€å®¶åº­ç­‰ã€‚
6. **æ—¶é—´**ï¼šç™½å¤©ã€é»‘å¤œç­‰ã€‚
7. **å“ç‰Œå‹å·**ï¼š{å“ç‰Œ}""",
            
            "æ— éœ€è§£æç»´åº¦": ""
        }
        
        selected_template = st.selectbox(
            "é€‰æ‹©è§£æç»´åº¦æ¨¡æ¿", 
            options=list(dimension_templates.keys()),
            key="batch_template_select"
        )
        
        default_dimensions = dimension_templates[selected_template]
        
        # è‡ªå®šä¹‰è§£æç»´åº¦
        custom_dimensions = st.text_area(
            "è‡ªå®šä¹‰è§†é¢‘è§£æç»´åº¦", 
            value=default_dimensions,
            height=150,
            key="batch_custom_dims",
            help="æ¯è¡Œä¸€ä¸ªè§£æè§’åº¦ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨æ›¿æ¢å…¶ä¸­çš„{è½¦å‹}æˆ–{å“ç‰Œ}å ä½ç¬¦ã€‚"
        )
        
        # å“ç‰Œè½¦å‹è¾“å…¥
        brand_model = st.text_input(
            "å“ç‰Œè½¦å‹/äº§å“", 
            value=st.session_state.config['GeminiService']['prompt'],
            placeholder="ä¾‹å¦‚ï¼šç†æƒ³L9ã€åä¸ºMate60ã€å°ç±³14ç­‰", 
            key="batch_brand_model",
            help="å°†æ›¿æ¢è§£æç»´åº¦ä¸­çš„{è½¦å‹}æˆ–{å“ç‰Œ}å ä½ç¬¦"
        )
        
        # å¹¶å‘å¤„ç†æ•°
        concurrency = st.slider(
            "å¹¶å‘å¤„ç†æ•°", 
            min_value=1, 
            max_value=7, 
            value=3, 
            help="åŒæ—¶å¤„ç†çš„è§†é¢‘æ•°é‡ï¼Œæœ€å¤§å€¼ä¸º7"
        )
        
        # è‡ªåŠ¨åˆ†ç‰‡é€‰é¡¹
        auto_split = st.checkbox(
            "è‡ªåŠ¨åˆ†ç‰‡å¤„ç†å¤§æ–‡ä»¶", 
            value=True,
            help=f"è‡ªåŠ¨å°†è¶…è¿‡{CHUNK_SIZE_MB}MBçš„è§†é¢‘åˆ†å‰²æˆè¾ƒå°çš„ç‰‡æ®µè¿›è¡Œå¤„ç†"
        )
        
        # ä»»åŠ¡æäº¤æŒ‰é’®
        submit = st.form_submit_button("å¼€å§‹æ‰¹é‡å¤„ç†", disabled=st.session_state.batch_processing)
    
    # å¦‚æœæäº¤äº†è¡¨å•
    if submit:
        if not knowledge_base_id:
            st.error("è¯·è¾“å…¥çŸ¥è¯†åº“ID")
        elif not uploaded_files:
            st.error("è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªè§†é¢‘æ–‡ä»¶")
        else:
            # æ›¿æ¢å ä½ç¬¦
            if "{è½¦å‹}" in custom_dimensions:
                custom_dimensions = custom_dimensions.replace("{è½¦å‹}", brand_model)
            if "{å“ç‰Œ}" in custom_dimensions:
                custom_dimensions = custom_dimensions.replace("{å“ç‰Œ}", brand_model)
            
            # æ›´æ–°é…ç½®
            st.session_state.config['KnowledgeBase']['datasetId'] = knowledge_base_id
            st.session_state.config['GeminiService']['prompt'] = brand_model
            
            # å‡†å¤‡æ‰¹å¤„ç†
            batch_tasks = []
            
            with st.spinner(f"å‡†å¤‡å¤„ç† {len(uploaded_files)} ä¸ªè§†é¢‘æ–‡ä»¶..."):
                # ä¿å­˜æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶å¹¶å¤„ç†å¤§æ–‡ä»¶åˆ†ç‰‡
                all_files_to_process = []
                
                for uploaded_file in uploaded_files:
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    original_file_path = save_uploaded_file(uploaded_file)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦éœ€è¦åˆ†ç‰‡
                    need_split, file_size = check_file_size(original_file_path)
                    
                    if need_split and auto_split:
                        st.info(f"æ–‡ä»¶ '{uploaded_file.name}' å¤§å°ä¸º {file_size/1024/1024:.1f}MBï¼Œè¶…è¿‡{CHUNK_SIZE_MB}MBï¼Œå°†è‡ªåŠ¨åˆ†ç‰‡å¤„ç†")
                        
                        try:
                            # åˆ†å‰²è§†é¢‘
                            chunk_files = split_video_file(original_file_path, uploaded_file.name)
                            
                            # æ·»åŠ æ‰€æœ‰åˆ†ç‰‡åˆ°å¤„ç†åˆ—è¡¨
                            for chunk_name, chunk_path in chunk_files:
                                all_files_to_process.append((chunk_name, chunk_path, True))
                                
                            st.success(f"å·²å°† '{uploaded_file.name}' åˆ†å‰²ä¸º {len(chunk_files)} ä¸ªç‰‡æ®µ")
                            
                        except Exception as e:
                            st.error(f"åˆ†å‰²è§†é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                            st.warning(f"å°†å°è¯•å¤„ç†åŸå§‹æ–‡ä»¶: {uploaded_file.name}")
                            all_files_to_process.append((uploaded_file.name, original_file_path, False))
                    else:
                        # å¦‚æœä¸éœ€è¦åˆ†ç‰‡æˆ–ä¸è‡ªåŠ¨åˆ†ç‰‡ï¼Œç›´æ¥å¤„ç†åŸå§‹æ–‡ä»¶
                        all_files_to_process.append((uploaded_file.name, original_file_path, False))
                
                # è®¾ç½®æ‰¹å¤„ç†æ ‡å¿—
                st.session_state.batch_processing = True
                
                # åˆ›å»ºä»»åŠ¡
                for i, (file_name, file_path, is_chunk) in enumerate(all_files_to_process):
                    # ç”Ÿæˆå”¯ä¸€ID
                    processing_uuid = str(uuid.uuid4())
                    
                    # åˆ›å»ºä»»åŠ¡ä¿¡æ¯
                    task_info = {
                        'file_name': file_name,
                        'file_path': file_path,
                        'knowledge_base_id': knowledge_base_id,
                        'custom_dimensions': custom_dimensions if custom_dimensions.strip() else None,
                        'processing_uuid': processing_uuid,
                        'status': 'PENDING',
                        'task_id': None,
                        'start_time': None,
                        'end_time': None,
                        'is_chunk': is_chunk  # æ ‡è®°æ˜¯å¦ä¸ºåˆ†ç‰‡
                    }
                    
                    # æ·»åŠ åˆ°ä»»åŠ¡åˆ—è¡¨
                    batch_tasks.append(task_info)
                
                # ä¿å­˜ä»»åŠ¡åˆ—è¡¨
                st.session_state.batch_tasks = batch_tasks
                
                st.success(f"æˆåŠŸå‡†å¤‡ {len(batch_tasks)} ä¸ªä»»åŠ¡ï¼Œå°†ä»¥ {concurrency} ä¸ªå¹¶å‘æ‰§è¡Œ")
                st.session_state.batch_concurrency = concurrency

# ä»»åŠ¡åˆ—è¡¨å’Œå¤„ç†é€»è¾‘
if st.session_state.batch_processing:
    # æ·»åŠ æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®å’Œä»»åŠ¡çŠ¶æ€æŒ‡ç¤ºå™¨
    col_task_header1, col_task_header2 = st.columns([5, 1])
    with col_task_header1:
        st.subheader("ä»»åŠ¡åˆ—è¡¨")
    with col_task_header2:
        if st.button("åˆ·æ–°çŠ¶æ€", key="refresh_status"):
            # å¼ºåˆ¶åˆ·æ–°é¡µé¢
            st.rerun()
    
    # æ›´æ–°æ‰€æœ‰ä»»åŠ¡çŠ¶æ€ï¼ˆä»Celeryè·å–æœ€æ–°çŠ¶æ€ï¼‰
    for i, task in enumerate(st.session_state.batch_tasks):
        if task['task_id']:
            new_status, progress_info = get_task_status(task['task_id'])
            # å¦‚æœçŠ¶æ€å˜åŒ–ï¼Œæ›´æ–°å½“å‰çŠ¶æ€
            if new_status != task['status']:
                # è®°å½•ä¹‹å‰çš„çŠ¶æ€
                old_status = task['status']
                # æ›´æ–°çŠ¶æ€
                task['status'] = new_status
                
                # è®°å½•è¿›åº¦ä¿¡æ¯
                if progress_info:
                    task['progress_info'] = progress_info
                
                # å¦‚æœä»»åŠ¡å®Œæˆ(ä»PROGRESS/STARTEDå˜ä¸ºSUCCESS/FAILURE)ï¼Œè®°å½•ç»“æŸæ—¶é—´
                if new_status in ['SUCCESS', 'FAILURE'] and old_status not in ['SUCCESS', 'FAILURE']:
                    task['end_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    # æ›´æ–°å†å²è®°å½•
                    history_entry = {
                        'task_id': task['task_id'],
                        'video_name': task['file_name'],
                        'knowledge_base_id': task['knowledge_base_id'],
                        'brand_model': st.session_state.config['GeminiService']['prompt'],
                        'start_time': task['start_time'],
                        'end_time': task['end_time'],
                        'status': new_status,
                        'processing_uuid': task['processing_uuid'],
                        'is_chunk': task.get('is_chunk', False)  # æ·»åŠ åˆ†ç‰‡æ ‡è®°
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    updated = False
                    for j, existing_task in enumerate(st.session_state.batch_task_history):
                        if existing_task.get('task_id') == task['task_id']:
                            st.session_state.batch_task_history[j] = history_entry
                            updated = True
                            break
                    
                    if not updated:
                        st.session_state.batch_task_history.append(history_entry)
                    
                    # ä¿å­˜åˆ°æ–‡ä»¶
                    save_task_history_to_file(st.session_state.batch_task_history)
                
                # æ›´æ–°session_state
                st.session_state.batch_tasks[i] = task
    
    # ä»»åŠ¡çŠ¶æ€è®¡æ•°
    pending_count = len([t for t in st.session_state.batch_tasks if t['status'] == 'PENDING'])
    running_count = len([t for t in st.session_state.batch_tasks if t['status'] in ['STARTED', 'PROGRESS']])
    completed_count = len([t for t in st.session_state.batch_tasks if t['status'] in ['SUCCESS', 'FAILURE']])
    total_count = len(st.session_state.batch_tasks)
    
    # æ˜¾ç¤ºç®€æ´çš„ä»»åŠ¡è¿›åº¦
    st.write(f"çŠ¶æ€: ç­‰å¾…ä¸­ {pending_count} | å¤„ç†ä¸­ {running_count} | å·²å®Œæˆ {completed_count} | æ€»è®¡ {total_count}")
    
    # æ˜¾ç¤ºè¿›åº¦æ¡
    progress = completed_count / total_count if total_count > 0 else 0
    st.progress(progress)
    
    # å‡†å¤‡ä»»åŠ¡æ•°æ®
    task_data = []
    for i, task in enumerate(st.session_state.batch_tasks):
        status = task['status']
        
        # ç¾åŒ–çŠ¶æ€æ˜¾ç¤º
        status_display = status
        if status == 'SUCCESS':
            status_display = "âœ… æˆåŠŸ"
        elif status == 'FAILURE':
            status_display = "âŒ å¤±è´¥"
        elif status == 'PENDING':
            status_display = "â³ ç­‰å¾…ä¸­"
        elif status == 'STARTED':
            status_display = "ğŸ”„ å¤„ç†ä¸­"
        elif status == 'PROGRESS':
            # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            progress_info = task.get('progress_info', {})
            if isinstance(progress_info, dict) and 'current' in progress_info and 'total' in progress_info:
                progress_percent = int((progress_info['current'] / progress_info['total']) * 100)
                status_display = f"ğŸ”„ å¤„ç†ä¸­ ({progress_percent}%)"
            else:
                status_display = "ğŸ”„ å¤„ç†ä¸­"
        
        # æ–‡ä»¶åæ˜¾ç¤ºï¼ˆæ·»åŠ åˆ†ç‰‡æ ‡è®°ï¼‰
        file_name_display = task['file_name']
        if task.get('is_chunk', False):
            file_name_display = f"{file_name_display} (åˆ†ç‰‡)"
        
        # æ·»åŠ ä»»åŠ¡ä¿¡æ¯
        task_data.append({
            "åºå·": i + 1,
            "æ–‡ä»¶å": file_name_display,
            "çŠ¶æ€": status_display,
            "å¼€å§‹æ—¶é—´": task['start_time'] or '-',
            "ç»“æŸæ—¶é—´": task['end_time'] or '-',
            "è¯¦æƒ…": task.get('progress_info', {}).get('step', '') if status == 'PROGRESS' else ''
        })
    
    # æ˜¾ç¤ºä»»åŠ¡è¡¨æ ¼
    task_df = pd.DataFrame(task_data)
    st.dataframe(task_df, use_container_width=True)
    
    # å¤„ç†ä»»åŠ¡é€»è¾‘
    if st.session_state.batch_processing:
        # è·å–å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡æ•°
        current_running = len([t for t in st.session_state.batch_tasks 
                              if t['status'] in ['STARTED', 'PROGRESS'] and t['task_id'] is not None])
        
        # è·å–æœ€å¤§å¹¶å‘æ•°
        max_concurrency = st.session_state.batch_concurrency
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç­‰å¾…å¤„ç†çš„ä»»åŠ¡
        pending_tasks = [i for i, t in enumerate(st.session_state.batch_tasks) 
                        if t['status'] == 'PENDING' and t['task_id'] is None]
        
        # å¦‚æœæœ‰ç­‰å¾…çš„ä»»åŠ¡ä¸”å½“å‰å¤„ç†ä»»åŠ¡æ•°å°äºæœ€å¤§å¹¶å‘æ•°
        if pending_tasks and current_running < max_concurrency:
            # è®¡ç®—å¯ä»¥å¯åŠ¨çš„ä»»åŠ¡æ•°
            to_start = min(len(pending_tasks), max_concurrency - current_running)
            
            for i in range(to_start):
                # è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡ç´¢å¼•
                task_idx = pending_tasks[i]
                task = st.session_state.batch_tasks[task_idx]
                st.write("å°è¯•å‘é€ä»»åŠ¡...")
                try:
                    # æ˜¾å¼å»ºç«‹è¿æ¥å¹¶å‘é€ä»»åŠ¡
                    with app.connection_or_acquire() as connection:
                        # å‘é€ä»»åŠ¡
                        celery_task = app.send_task(
                            'tasks.process_video_task',
                            args=[
                                task['file_path'],
                                task['knowledge_base_id'],
                                task['custom_dimensions'],
                                task['file_name'],
                                task['processing_uuid']
                            ],
                            connection=connection  # æ˜¾å¼æŒ‡å®šè¿æ¥
                        )
                        
                        st.success(f"ä»»åŠ¡å·²å‘é€! ID: {celery_task.id}")
                        
                except Exception as e:
                    st.error(f"å‘é€ä»»åŠ¡æ—¶å‡ºé”™: {e}")
                    import traceback
                    st.error(traceback.format_exc())
                    # æ ‡è®°ä»»åŠ¡å¤±è´¥
                    task['status'] = 'FAILURE'
                    task['end_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    continue  # è·³è¿‡æ­¤ä»»åŠ¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                
                # æ›´æ–°ä»»åŠ¡ä¿¡æ¯
                task['task_id'] = celery_task.id
                task['status'] = 'PENDING'  # åˆå§‹çŠ¶æ€ä¸ºPENDINGï¼ŒCeleryä¼šæ›´æ–°ä¸ºSTARTED
                task['start_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                # åˆ›å»ºä»»åŠ¡å†å²è®°å½•
                history_entry = {
                    'task_id': celery_task.id,
                    'video_name': task['file_name'],
                    'knowledge_base_id': task['knowledge_base_id'],
                    'brand_model': st.session_state.config['GeminiService']['prompt'],
                    'start_time': task['start_time'],
                    'status': 'PENDING',
                    'processing_uuid': task['processing_uuid'],
                    'is_chunk': task.get('is_chunk', False)  # æ·»åŠ åˆ†ç‰‡æ ‡è®°
                }
                save_task_history(history_entry)
                
                st.session_state.batch_tasks[task_idx] = task
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²ç»“æŸ
    if all(task['status'] not in ['STARTED', 'PROGRESS', 'PENDING'] for task in st.session_state.batch_tasks):
        # å¦‚æœä¹‹å‰åœ¨å¤„ç†ä¸­ï¼Œç°åœ¨æ ‡è®°ä¸ºå·²å®Œæˆ
        if st.session_state.batch_processing:
            st.session_state.batch_processing = False
            st.success("æ‰€æœ‰æ‰¹å¤„ç†ä»»åŠ¡å·²å®Œæˆ")
            st.rerun()

# å†å²æ‰¹å¤„ç†è®°å½•
st.subheader("å†å²æ‰¹å¤„ç†è®°å½•")

# æ·»åŠ æ¸…ç©ºå†å²æŒ‰é’®
col_history_header1, col_history_header2 = st.columns([5, 1])
with col_history_header2:
    if st.button("æ¸…ç©ºå†å²è®°å½•", key="clear_history"):
        if st.session_state.batch_task_history:
            st.session_state.batch_task_history = []
            save_task_history_to_file([])
            st.success("å†å²è®°å½•å·²æ¸…ç©º")
            st.rerun()

if st.session_state.batch_task_history:
    # å‡†å¤‡å†å²æ•°æ®
    history_data = []
    for task in st.session_state.batch_task_history:
        status = task.get('status', '')
        # ç¾åŒ–çŠ¶æ€æ˜¾ç¤º
        status_display = status
        if status == 'SUCCESS':
            status_display = "âœ… æˆåŠŸ"
        elif status == 'FAILURE':
            status_display = "âŒ å¤±è´¥"
        elif status == 'PENDING':
            status_display = "â³ ç­‰å¾…ä¸­"
        elif status == 'STARTED' or status == 'PROGRESS':
            status_display = "ğŸ”„ å¤„ç†ä¸­"
        
        # æ–‡ä»¶åæ˜¾ç¤ºï¼ˆæ·»åŠ åˆ†ç‰‡æ ‡è®°ï¼‰
        video_name = task.get('video_name', '')
        if task.get('is_chunk', False):
            video_name = f"{video_name} (åˆ†ç‰‡)"
            
        history_data.append({
            "è§†é¢‘åç§°": video_name,
            "å¼€å§‹æ—¶é—´": task.get('start_time', ''),
            "ç»“æŸæ—¶é—´": task.get('end_time', '-'),
            "çŠ¶æ€": status_display,
            "çŸ¥è¯†åº“ID": task.get('knowledge_base_id', ''),
            "ä»»åŠ¡ID": task.get('task_id', '')
        })
    
    # æ˜¾ç¤ºå†å²è®°å½•è¡¨æ ¼
    history_df = pd.DataFrame(history_data)
    st.dataframe(history_df, use_container_width=True)
    
    # æ˜¾ç¤ºå†å²è®°å½•ç»Ÿè®¡
    st.info(f"å…±æœ‰ {len(history_data)} æ¡å†å²è®°å½•ï¼Œ"
           f"å…¶ä¸­æˆåŠŸ {len([t for t in st.session_state.batch_task_history if t.get('status') == 'SUCCESS'])} æ¡ï¼Œ"
           f"å¤±è´¥ {len([t for t in st.session_state.batch_task_history if t.get('status') == 'FAILURE'])} æ¡")
else:
    st.info("æš‚æ— å†å²æ‰¹å¤„ç†è®°å½•")

# è‡ªåŠ¨åˆ·æ–°æ‰¹å¤„ç†çŠ¶æ€
if st.session_state.batch_processing:
    time.sleep(1)  # å°å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«åˆ·æ–°
    st.rerun()

# åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.caption("æ‰¹é‡å¤„ç†é¡µé¢æ”¯æŒä¸Šä¼ å¤šä¸ªè§†é¢‘æ–‡ä»¶å¹¶åŒæ—¶å¤„ç†ï¼Œå¯ä»¥æ§åˆ¶å¹¶å‘å¤„ç†çš„æ•°é‡ã€‚")
st.caption("å¤„ç†è¿‡ç¨‹ä¸­å¯ä»¥éšæ—¶æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€å’Œè¿›åº¦ã€‚")
st.caption(f"æ”¯æŒè‡ªåŠ¨åˆ†ç‰‡å¤„ç†è¶…è¿‡{CHUNK_SIZE_MB}MBçš„å¤§å‹è§†é¢‘æ–‡ä»¶ã€‚") 